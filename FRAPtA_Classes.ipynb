{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <H1> FRAPtrackAnalyser Classes </H1> </center>\n",
    "\n",
    "    \n",
    "            *Code written by Emilie Cuillery and Timo Rey. Laboratory of Experimental Biophysics, EPFL*\n",
    "                                            *Created between 2018 - 2020*\n",
    "#### Aims:\n",
    "    Streamline FRAPtA_Analysis scripts.\n",
    "#### Use of script:\n",
    "    Save in same directory as FRAPtA_Analysis scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries:\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exponential types:\n",
    "# single exponential\n",
    "def single_exp(x, a, b):\n",
    "    return a*(1-np.exp(-b*x))\n",
    "\n",
    "# double exponential\n",
    "def double_exp(x, a, b, c, d):\n",
    "    return a*(1-np.exp(-b*x))+c*(1-np.exp(-d*x))\n",
    "\n",
    "# set default:\n",
    "single = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**\n",
    "\n",
    "    each experimental class contains the following defining values:\n",
    "\n",
    "    Condition_name = FRAPcondition(\n",
    "    r'Condition_name',                                               #Name\n",
    "    r'Path',                                                         #Path\n",
    "    'Subset',                                                        #Subset\n",
    "    Nb_Frames,                                                       #Nb_Frames\n",
    "    Prebleach_Frames,                                                #Preableach_Steps\n",
    "    SecondsPerFrame                                                  #after bleach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class \"FRAPcondition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRAPcondition:\n",
    "    \"\"\" Create a new condition, defined by several parameters \"\"\"\n",
    "\n",
    "                        #{0}  #{1}  #{2}     #{3}       #{4} \n",
    "    def __init__(self, Name, Path, Subset, Nb_Frames, Prebleach_Frames, SecondsPerFrame):\n",
    "\n",
    "        self.Name = Name\n",
    "        self.Path = Path\n",
    "        self.Subset = Subset\n",
    "        self.Nb_Frames = Nb_Frames\n",
    "        self.Prebleach_Frames = Prebleach_Frames\n",
    "        self.SecondsPerFrame = SecondsPerFrame\n",
    "\n",
    "        self.txt_List = None\n",
    "        self.Data = pd.DataFrame()\n",
    "        self.Mean = []\n",
    "        self.Std = []\n",
    "        self.Mobile_Fraction_ave = None #Mobile fraction calculated by averaging all the data\n",
    "        self.Mobile_Fraction = []\n",
    "        self.Half_Time_ave = None #Half Time calculated by averaging all the data\n",
    "        self.Half_Time = []\n",
    "    \n",
    "        print(\"Condition called '{0}' was successfully created\".format(self.Name))   \n",
    "\n",
    "\n",
    "    def __str__(self): #can call this with print(self)\n",
    "        return \"\"\"Condition:\n",
    " Name: {0} \\n Path: {1} \\n Subset: {2} \\n Number of frames analyzed: {3} \\n Number of pre-bleach frames: {4}\n",
    " Seconds per frame: {5}s\"\"\".format(self.Name, self.Path, self.Subset, self.Nb_Frames, self.Prebleach_Frames, self.SecondsPerFrame)\n",
    "    \n",
    "    def Extract_Data(self):\n",
    "        \"\"\" Find .txt files and read them to extract data. Also compute normalisedFS data \"\"\"\n",
    "        \n",
    "        print(\"Find data for \", self.Name)\n",
    "        \n",
    "        #PART 1 - Get txt_list                                                                      \n",
    "        self.txt_List = sorted(Path(self.Path).glob('*' + self.Subset + '*.txt'))        \n",
    "        # Get Data\n",
    "        Parser = 0\n",
    "        print(\"Extract data on\",self.Nb_Frames, \"frames and for\",len(self.txt_List) ,\"files\")\n",
    "        \n",
    "        while Parser < len(self.txt_List):           \n",
    "            #Read file and get Raw_Data\n",
    "            with open(str(self.txt_List[Parser]), 'r') as file:                         \n",
    "                Raw_Data= pd.read_csv(file, sep = '\\t')                            \n",
    "                Raw_Data.columns = ['FRAME','TIME', 'FRAP', 'REF', 'BACK']\n",
    "                if Parser==0:\n",
    "                    self.Data['FRAME'] = pd.Series(Raw_Data['FRAME'].iloc[:self.Nb_Frames])\n",
    "                    self.Data['TIME'] = pd.Series(Raw_Data['TIME'].iloc[:self.Nb_Frames])\n",
    "            \n",
    "        #PART 2 - Normalise FRAP Intensities\n",
    "            Prebleach = Raw_Data[:int(self.Prebleach_Frames)]                    #Get pre-bleach data                                                   \n",
    "           \n",
    "            CorrPreREF = Prebleach['REF']-Prebleach['BACK']                      #Compute parameters\n",
    "            Iref_pre = CorrPreREF.mean()\n",
    "            CorrPreFRAP = Prebleach['FRAP']-Prebleach['BACK']\n",
    "            Ifrap_pre = CorrPreFRAP.mean()\n",
    "            \n",
    "            Ifrap = Raw_Data['FRAP']\n",
    "            Iref = Raw_Data['REF']\n",
    "            Iback = Raw_Data['BACK']\n",
    "            \n",
    "            Inorm = (Iref_pre / (Iref - Iback)) * ((Ifrap - Iback) / Ifrap_pre)  #Double normalisation\n",
    "                    #see FRAPAnalyser here: http://actinsim.uni.lu/eng/Downloads for more insights \n",
    "    \n",
    "            Inorm_pre = 0                                                        #Full scaling starts\n",
    "            for i in range(self.Prebleach_Frames):\n",
    "                Inorm_pre = Inorm_pre + Inorm[i]\n",
    "            Inorm_pre = Inorm_pre / self.Prebleach_Frames\n",
    "            Inorm_FULLSCALE = (Inorm - Inorm[self.Prebleach_Frames])/(Inorm_pre - Inorm[self.Prebleach_Frames])\n",
    "    \n",
    "            Raw_Data['NormalizedFS'] = pd.Series(Inorm_FULLSCALE)\n",
    "        \n",
    "            #Add df to DATA\n",
    "            self.Data['#'+str(Parser)+'_FRAP_'+str(self.Subset)] = pd.Series(Raw_Data['FRAP'].iloc[:self.Nb_Frames])\n",
    "            self.Data['#'+str(Parser)+'_REF_'+str(self.Subset)] = pd.Series(Raw_Data['REF'].iloc[:self.Nb_Frames])\n",
    "            self.Data['#'+str(Parser)+'_BACK_'+str(self.Subset)] = pd.Series(Raw_Data['BACK'].iloc[:self.Nb_Frames])\n",
    "            self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)] = pd.Series(Raw_Data['NormalizedFS'].iloc[:self.Nb_Frames])\n",
    "            \n",
    "            Parser += 1\n",
    "        print(\"Raw data extracted and normalised.\")\n",
    "\n",
    "        \n",
    "    def Mean_and_Std(self):\n",
    "        \"\"\" Compute Mean and Standard deviation \"\"\"            \n",
    "        \n",
    "        tmp = [0]*len(self.txt_List) # creates array with y values for each time point (length txtList)\n",
    "\n",
    "        for i in range(0,self.Nb_Frames):\n",
    "            for Parser in range(0,len(self.txt_List)):\n",
    "                tmp[Parser] = self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)][i]\n",
    "            self.Mean.append(np.mean(tmp))\n",
    "            self.Std.append(np.std(tmp))\n",
    "        \n",
    "        print(\"Mean and Sandard deviation were successfully computed.\")\n",
    "\n",
    "        \n",
    "    def FitData(self):\n",
    "        \"\"\" Fit all normalised recovery curves and plot fit with std \"\"\"\n",
    "    \n",
    "        for Parser in range(0,len(self.txt_List)):\n",
    "            x=self.Data['TIME'][0:self.Nb_Frames-int(self.Prebleach_Frames)]-1\n",
    "            y=self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)][int(self.Prebleach_Frames):self.Nb_Frames]\n",
    "            Residues = np.zeros(len(y))\n",
    "        \n",
    "            popt, pcov = curve_fit(single_exp, x, y)\n",
    "    \n",
    "            #Store values\n",
    "            self.Mobile_Fraction.append(popt[0])\n",
    "            self.Half_Time.append(-1*np.log(0.5)/(popt[1]/self.SecondsPerFrame))\n",
    "            \n",
    "        print(str(len(self.txt_List)) + \" data was fit as well as MF & HT computed.\")\n",
    "\n",
    "\n",
    "    def PlotFitWerror(self, outDir=None, single=False, color1 = 'black', color2 = 'forestgreen', color3  = 'peru', \n",
    "                     xAxis = r'time [s]', yAxis = 'norm. intensity'):\n",
    "        \"\"\" Fit all normalised recovery curves and plot fit with std \"\"\"\n",
    "        \n",
    "        #choosing single or double exponential fit as a model:\n",
    "        if single == True:\n",
    "            exponential = single_exp\n",
    "            print(\"you are fitting with a single exponential\")\n",
    "        elif single == False:\n",
    "            exponential = double_exp\n",
    "            print(\"you are fitting with a double exponential\")\n",
    "        \n",
    "        #Redo the fit:   \n",
    "        y_mean_plus = []                                                             #upper bound\n",
    "        for i in range(0,self.Nb_Frames):\n",
    "            y_mean_plus.append(self.Mean[i] + self.Std[i])  \n",
    "        y_mean_minus = []                                                            #lower bound \n",
    "        for i in range(0,self.Nb_Frames):\n",
    "            y_mean_minus.append(self.Mean[i] - self.Std[i])\n",
    "\n",
    "        x=self.Data['TIME'][0:self.Nb_Frames-int(self.Prebleach_Frames)]-1\n",
    "        y=self.Mean[int(self.Prebleach_Frames):self.Nb_Frames]\n",
    "        popt, pcov = curve_fit(exponential, x, y, p0=None)\n",
    "        \n",
    "        xx = np.linspace(0,self.Nb_Frames,1000)\n",
    "        yy = exponential(xx, *popt)\n",
    "\n",
    "        xxx = (self.Data['TIME'] - self.Data['TIME'][self.Prebleach_Frames])        \n",
    "        \n",
    "        #output the fit:\n",
    "        ax0 = plt.subplot2grid((3, 3), (0, 0), colspan=3, rowspan=4)                         #define geometry of the plot\n",
    "\n",
    "        ax0.plot(xxx, self.Mean[:], 'b-', color = color1, label = 'Averaged FRAP recovery')  #Mean\n",
    "        ax0.plot(xxx, y_mean_plus[:], \"y--\", color = color2)                                 #std+\n",
    "        ax0.plot(xxx, y_mean_minus[:], \"y--\", color = color2)                                #std-\n",
    "        ax0.plot(xx, yy,'k-', color = color3)                                                #fit\n",
    "        ax0.set_xlim(xxx[0], xxx[self.Nb_Frames-1])\n",
    "        ax0.set_ylim([0, 1.2])\n",
    "        ax0.set_title(self.Name)\n",
    "        ax0.set_xlabel(xAxis)\n",
    "        ax0.set_ylabel(yAxis)\n",
    "        \n",
    "        if outDir != None:\n",
    "            plt.savefig(outDir + self.Name + \".svg\")\n",
    "            print(\"This plot was saved at: \" + outDir)\n",
    "        else:\n",
    "            print(\"This plot was not saved\")\n",
    " \n",
    "    def PlotIndFit(self):\n",
    "        \"\"\" Recalculate fits and plot individually \"\"\"\n",
    "    \n",
    "        for Parser in range(0,len(self.txt_List)):\n",
    "            x=(self.Data['FRAME'][0:self.Nb_Frames-int(self.Prebleach_Frames)]-1)*self.SecondsPerFrame\n",
    "            y=self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)][int(self.Prebleach_Frames):self.Nb_Frames]\n",
    "            Residues = np.zeros(len(y))\n",
    "            \n",
    "            popt, pcov = curve_fit(single_exp, x, y, p0=None)\n",
    "\n",
    "            xx = np.linspace(0,self.Nb_Frames,1000) #start, stop, num (number of samples to generate)\n",
    "            yy = single_exp(xx, *popt)\n",
    "\n",
    "            #Calculate Residues\n",
    "            yyy = single_exp(x, *popt)\n",
    "            Experimental_values = y.values.tolist() #conversion to array \n",
    "            Fitted_values = yyy.values.tolist() #conversion to array\n",
    "            for i in range(0, len(Experimental_values)):\n",
    "                Residues[i]=Experimental_values[i]-Fitted_values[i]\n",
    "        \n",
    "            #Subplot\n",
    "            f, axarr = plt.subplots(2, sharex=True)\n",
    "            axarr[0].plot(x,y,'x')\n",
    "            axarr[0].plot(xx*self.SecondsPerFrame, yy,'r-')\n",
    "            axarr[0].set_title(self.Name+', fitting to exponential')\n",
    "            axarr[1].plot(x, Residues,'kx')\n",
    "            axarr[1].plot([0,self.Nb_Frames], [0,0],'k--')\n",
    "            axarr[1].set_title(self.Name+ ', residues of the fitting')\n",
    "\n",
    "            print(\"Fitting results for #\",Parser ,\":\\nMobile fraction=\",round(popt[0],2), \"and half time = \", round(self.Half_Time[Parser],2)) #print results of the fitting (a and b)\n",
    "            print(\"Standard deviation of residues = \",round(np.std(Residues),3)) #print results of the fitting (a and b)\n",
    "\n",
    "            plt.xlim(0, max(x))\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "    def Plot_Data(self):\n",
    "        \"\"\" Plot Data of an object from class Experiment \"\"\"\n",
    "        \n",
    "        for Parser in range(0,len(self.txt_List)):\n",
    "        #for Parser in range(0,1):\n",
    "            x=self.Data['TIME']\n",
    "            y=self.Data['#'+str(Parser)+'_FRAP_'+str(self.Subset)]\n",
    "            z=self.Data['#'+str(Parser)+'_REF_'+str(self.Subset)]\n",
    "            w=self.Data['#'+str(Parser)+'_BACK_'+str(self.Subset)]\n",
    "            n=self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)]\n",
    "\n",
    "            plt.figure(figsize=(20,6))\n",
    "            \n",
    "            plt.subplot(121)\n",
    "            plt.plot(x,y,'x-',color = 'royalblue', label = 'FRAP (file#'+str(Parser)+')')\n",
    "            plt.plot(x,z,'--',color = 'slategray', label = 'Reference (file#'+str(Parser)+')')\n",
    "            plt.plot(x,w,'--',color = 'k', label = 'Background (file#'+str(Parser)+')')\n",
    "            plt.xlabel(r'Time [s]')\n",
    "            plt.ylabel(r'Intensity [AU] [0,255]')\n",
    "            pylab.title(self.Name + ', File #' +str(Parser))\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(122)\n",
    "            plt.plot(x,n,'x',color = 'seagreen', label = 'FRAP normalized(file#'+str(Parser)+')')\n",
    "            plt.xlabel(r'Time [s]')\n",
    "            plt.ylabel(r'Normalized Intensity [AU] [0,1]')\n",
    "            pylab.title(self.Name+ ', Normalized, File #' +str(Parser))\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def List(self):\n",
    "        \"\"\" output ordered list of all txt files used in analysis \"\"\" \n",
    "        for i in range(0,len(self.txt_List)):\n",
    "            print(i, \": \",self.txt_List[i],\"\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class was initiated\n"
     ]
    }
   ],
   "source": [
    "print(\"class was initiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRAPnoTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRAPnoTime:\n",
    "    \"\"\" For conditions without time-column in data-file \"\"\"\n",
    "    \n",
    "                        #{0}  #{1}  #{2}     #{3}       #{4} \n",
    "    def __init__(self, Name, Path, Subset, Nb_Frames, Prebleach_Frames, SecondsPerFrame):\n",
    "\n",
    "        self.Name = Name\n",
    "        self.Path = Path\n",
    "        self.Subset = Subset\n",
    "        self.Nb_Frames = Nb_Frames\n",
    "        self.Prebleach_Frames = Prebleach_Frames\n",
    "        self.SecondsPerFrame = SecondsPerFrame\n",
    "\n",
    "        self.txt_List = None\n",
    "        self.Data = pd.DataFrame()\n",
    "        self.Mean = []\n",
    "        self.Std = []\n",
    "        self.Mobile_Fraction_ave = None #Mobile fraction calculated by averaging all the data\n",
    "        self.Mobile_Fraction = []\n",
    "        self.Half_Time_ave = None #Half Time calculated by averaging all the data\n",
    "        self.Half_Time = []\n",
    "    \n",
    "        print(\"Condition called '{0}' was successfully created\".format(self.Name))   \n",
    "\n",
    "        \n",
    "    def __str__(self): #can call this with print(self)\n",
    "        return \"\"\"Condition:\n",
    " Name: {0} \\n Path: {1} \\n Subset: {2} \\n Number of frames analyzed: {3} \\n Number of pre-bleach frames: {4}\n",
    " Seconds per frame: {5}s\"\"\".format(self.Name, self.Path, self.Subset, self.Nb_Frames, self.Prebleach_Frames, self.SecondsPerFrame)\n",
    "\n",
    "    \n",
    "    def Extract_Data(self):\n",
    "        \"\"\" Find .txt files and read them to extract data. Also compute normalizedFS data \"\"\"\n",
    "        \n",
    "        print(\"Find data for \", self.Name)\n",
    "        #PART 1 - Get txt_list                                                                      \n",
    "        self.txt_List = sorted(Path(self.Path).glob('*' + self.Subset + '*.txt'))        \n",
    "        #       - Get Data\n",
    "        Parser = 0\n",
    "        print(\"Extract data on\",self.Nb_Frames, \"frames and for\",len(self.txt_List) ,\"files\")\n",
    "        \n",
    "        while Parser < len(self.txt_List):           \n",
    "            #Read file and get Raw_Data\n",
    "            with open(str(self.txt_List[Parser]), 'r') as file:                         \n",
    "                Raw_Data= pd.read_csv(file, sep = '\\t')                            \n",
    "                Raw_Data.columns = ['FRAME', 'FRAP', 'REF', 'BACK']\n",
    "                if Parser==0:\n",
    "                    self.Data['FRAME'] = pd.Series(Raw_Data['FRAME'].iloc[:self.Nb_Frames])\n",
    "            \n",
    "        #PART 2 - Normalise FRAP Intensities\n",
    "            Prebleach = Raw_Data[:int(self.Prebleach_Frames)]                    #Get pre-bleach data                                                   \n",
    "           \n",
    "            CorrPreREF = Prebleach['REF']-Prebleach['BACK']                      #Compute parameters\n",
    "            Iref_pre = CorrPreREF.mean()\n",
    "            CorrPreFRAP = Prebleach['FRAP']-Prebleach['BACK']\n",
    "            Ifrap_pre = CorrPreFRAP.mean()\n",
    "            \n",
    "            Ifrap = Raw_Data['FRAP']\n",
    "            Iref = Raw_Data['REF']\n",
    "            Iback = Raw_Data['BACK']\n",
    "            \n",
    "            Inorm = (Iref_pre / (Iref - Iback)) * ((Ifrap - Iback) / Ifrap_pre)  #Double normalisation\n",
    "                    #see FRAPAnalyser here: http://actinsim.uni.lu/eng/Downloads for more insights \n",
    "    \n",
    "            Inorm_pre = 0                                                        #Full scaling starts\n",
    "            for i in range(self.Prebleach_Frames):\n",
    "                Inorm_pre = Inorm_pre + Inorm[i]\n",
    "            Inorm_pre = Inorm_pre / self.Prebleach_Frames\n",
    "            Inorm_FULLSCALE = (Inorm - Inorm[self.Prebleach_Frames])/(Inorm_pre - Inorm[self.Prebleach_Frames])\n",
    "    \n",
    "            Raw_Data['NormalizedFS'] = pd.Series(Inorm_FULLSCALE)\n",
    "        \n",
    "            #Add df to DATA\n",
    "            self.Data['#'+str(Parser)+'_FRAP_'+str(self.Subset)] = pd.Series(Raw_Data['FRAP'].iloc[:self.Nb_Frames])\n",
    "            self.Data['#'+str(Parser)+'_REF_'+str(self.Subset)] = pd.Series(Raw_Data['REF'].iloc[:self.Nb_Frames])\n",
    "            self.Data['#'+str(Parser)+'_BACK_'+str(self.Subset)] = pd.Series(Raw_Data['BACK'].iloc[:self.Nb_Frames])\n",
    "            self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)] = pd.Series(Raw_Data['NormalizedFS'].iloc[:self.Nb_Frames])\n",
    "            \n",
    "            Parser += 1\n",
    "        \n",
    "        print(\"Raw data extracted and normalised.\")\n",
    "\n",
    "        \n",
    "    def Mean_and_Std(self):\n",
    "        \"\"\" Compute Mean and Standard deviation \"\"\"            \n",
    "        \n",
    "        tmp = [0]*len(self.txt_List) # creates array with y values for each time point (length txtList)\n",
    "\n",
    "        for i in range(0,self.Nb_Frames):\n",
    "            for Parser in range(0,len(self.txt_List)):\n",
    "                tmp[Parser] = self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)][i]\n",
    "            self.Mean.append(np.mean(tmp))\n",
    "            self.Std.append(np.std(tmp))\n",
    "        \n",
    "        print(\"Mean and Sandard deviation were successfully computed.\")\n",
    "\n",
    "        \n",
    "    def FitData(self):\n",
    "        \"\"\" Fit all normalised recovery curves and plot fit with std \"\"\"\n",
    "    \n",
    "        for Parser in range(0,len(self.txt_List)):\n",
    "            x=(self.Data['FRAME'][0:self.Nb_Frames-int(self.Prebleach_Frames)]-1)*self.SecondsPerFrame\n",
    "            y=self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)][int(self.Prebleach_Frames):self.Nb_Frames]\n",
    "            Residues = np.zeros(len(y))\n",
    "        \n",
    "            popt, pcov = curve_fit(single_exp, x, y)\n",
    "    \n",
    "            #Store values\n",
    "            self.Mobile_Fraction.append(popt[0])\n",
    "            self.Half_Time.append(-1*np.log(0.5)/(popt[1]/self.SecondsPerFrame))\n",
    "            \n",
    "        print(str(len(self.txt_List)) + \" data was fit as well as MF & HT computed.\")\n",
    "\n",
    "        \n",
    "\n",
    "    def PlotFitWerror(self, outDir=None, single=False, color1 = 'black', color2 = 'forestgreen', color3  = 'peru', \n",
    "                     xAxis = r'time [s]', yAxis = 'norm. intensity'):\n",
    "        \"\"\" Fit all normalised recovery curves and plot fit with std \"\"\"\n",
    "        \n",
    "        #choosing single or double exponential fit as a model:\n",
    "        if single == True:\n",
    "            exponential = single_exp\n",
    "            print(\"you are fitting with a single exponential\")\n",
    "        elif single == False:\n",
    "            exponential = double_exp\n",
    "            print(\"you are fitting with a double exponential\")\n",
    "        \n",
    "        #Redo the fit:   \n",
    "        y_mean_plus = []                                                             #upper bound\n",
    "        for i in range(0,self.Nb_Frames):\n",
    "            y_mean_plus.append(self.Mean[i] + self.Std[i])  \n",
    "        y_mean_minus = []                                                            #lower bound \n",
    "        for i in range(0,self.Nb_Frames):\n",
    "            y_mean_minus.append(self.Mean[i] - self.Std[i])\n",
    "\n",
    "        x=(self.Data['FRAME'][0:self.Nb_Frames-int(self.Prebleach_Frames)]-1)*self.SecondsPerFrame\n",
    "        y=self.Mean[int(self.Prebleach_Frames):self.Nb_Frames]\n",
    "        popt, pcov = curve_fit(exponential, x, y, p0=None)\n",
    "        \n",
    "        xx = np.linspace(0,self.Nb_Frames,1000)\n",
    "        yy = exponential(xx, *popt)\n",
    "\n",
    "        xxx = ((self.Data['FRAME'] - self.Data['FRAME'][self.Prebleach_Frames]))*self.SecondsPerFrame      \n",
    "        \n",
    "        #output the fit:\n",
    "        ax0 = plt.subplot2grid((3, 3), (0, 0), colspan=3, rowspan=4)                            #define geometry of the plot\n",
    "\n",
    "        ax0.plot(xxx, self.Mean[:], 'b-', color = color1, label = 'Averaged FRAP recovery')  #Mean\n",
    "        ax0.plot(xxx, y_mean_plus[:], \"y--\", color = color2)                                 #std+\n",
    "        ax0.plot(xxx, y_mean_minus[:], \"y--\", color = color2)                                #std-\n",
    "        ax0.plot(xx, yy,'k-', color = color3)                                                     #fit\n",
    "        ax0.set_xlim(xxx[0], xxx[self.Nb_Frames-1])\n",
    "        ax0.set_ylim([0, 1.2])\n",
    "        ax0.set_title(self.Name)\n",
    "        ax0.set_xlabel(xAxis)\n",
    "        ax0.set_ylabel(yAxis)\n",
    "        \n",
    "        if outDir != None:\n",
    "            plt.savefig(outDir + self.Name + \".svg\")\n",
    "            print(\"This plot was saved at: \" + outDir)\n",
    "        else:\n",
    "            print(\"This plot was not saved\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    def PlotIndFit(self):\n",
    "        \"\"\" Recalculate fits and plot individually \"\"\"\n",
    "    \n",
    "        for Parser in range(0,len(self.txt_List)):\n",
    "            x=(self.Data['FRAME'][0:self.Nb_Frames-int(self.Prebleach_Frames)]-1)*self.SecondsPerFrame\n",
    "            y=self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)][int(self.Prebleach_Frames):self.Nb_Frames]\n",
    "            Residues = np.zeros(len(y))\n",
    "            \n",
    "            popt, pcov = curve_fit(single_exp, x, y, p0=None)\n",
    "\n",
    "            xx = np.linspace(0,self.Nb_Frames,1000) #start, stop, num (number of samples to generate)\n",
    "            yy = single_exp(xx, *popt)\n",
    "\n",
    "            #Calculate Residues\n",
    "            yyy = single_exp(x, *popt)\n",
    "            Experimental_values = y.values.tolist() #conversion to array \n",
    "            Fitted_values = yyy.values.tolist() #conversion to array\n",
    "            for i in range(0, len(Experimental_values)):\n",
    "                Residues[i]=Experimental_values[i]-Fitted_values[i]\n",
    "        \n",
    "            #Subplot\n",
    "            f, axarr = plt.subplots(2, sharex=True)\n",
    "            axarr[0].plot(x,y,'x')\n",
    "            axarr[0].plot(xx*self.SecondsPerFrame, yy,'r-')\n",
    "            axarr[0].set_title(self.Name+', fitting to exponential')\n",
    "            axarr[1].plot(x, Residues,'kx')\n",
    "            axarr[1].plot([0,self.Nb_Frames], [0,0],'k--')\n",
    "            axarr[1].set_title(self.Name+ ', residues of the fitting')\n",
    "\n",
    "            print(\"Fitting results for #\",Parser ,\":\\nMobile fraction=\",round(popt[0],2), \"and half time = \", round(self.Half_Time[Parser],2)) #print results of the fitting (a and b)\n",
    "            print(\"Standard deviation of residues = \",round(np.std(Residues),3)) #print results of the fitting (a and b)\n",
    "\n",
    "            plt.xlim(0, max(x))\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "    def Plot_Data(self):\n",
    "        \"\"\" Plot Data of an object from class Experiment \"\"\"\n",
    "        \n",
    "        for Parser in range(0,len(self.txt_List)):\n",
    "        #for Parser in range(0,1):\n",
    "            x=(self.Data['FRAME'])*self.SecondsPerFrame\n",
    "            y=self.Data['#'+str(Parser)+'_FRAP_'+str(self.Subset)]\n",
    "            z=self.Data['#'+str(Parser)+'_REF_'+str(self.Subset)]\n",
    "            w=self.Data['#'+str(Parser)+'_BACK_'+str(self.Subset)]\n",
    "            n=self.Data['#'+str(Parser)+'_NormalizedFS_'+str(self.Subset)]\n",
    "\n",
    "            plt.figure(figsize=(20,6))\n",
    "            \n",
    "            plt.subplot(121)\n",
    "            plt.plot(x,y,'x-',color = 'royalblue', label = 'FRAP (file#'+str(Parser)+')')\n",
    "            plt.plot(x,z,'--',color = 'slategray', label = 'Reference (file#'+str(Parser)+')')\n",
    "            plt.plot(x,w,'--',color = 'k', label = 'Background (file#'+str(Parser)+')')\n",
    "            plt.xlabel(r'Time [s]')\n",
    "            plt.ylabel(r'Intensity [AU] [0,255]')\n",
    "            pylab.title(self.Name + ', File #' +str(Parser))\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(122)\n",
    "            plt.plot(x,n,'x',color = 'seagreen', label = 'FRAP normalized(file#'+str(Parser)+')')\n",
    "            plt.xlabel(r'Time [s]')\n",
    "            plt.ylabel(r'Normalized Intensity [AU] [0,1]')\n",
    "            pylab.title(self.Name+ ', Normalized, File #' +str(Parser))\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "    def List(self):\n",
    "        \"\"\" output ordered list of all txt files used in analysis \"\"\" \n",
    "        for i in range(0,len(self.txt_List)):\n",
    "            print(i, \": \",self.txt_List[i],\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
